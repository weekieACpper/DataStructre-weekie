# 静态散列表(static HashTable)

## 一、一些学习此内容所需要的准备

### 1 需要提前了解的知识	

​	1、了解数论方面的知识(非必需)。

​	2、了解离散数学方面的知识(非必需)。

​	3、熟悉概率论。

​	4、熟悉数学归纳法。

​	5、熟悉微积分。

​	6、熟悉链表。

​	7、时间复杂度的计算，以及记号。

​	8、熟悉c或c++的基本语法。

​	9、数学期望的线性性质。

## 二、直接寻址表

​		我们一较为简单的模型开始，慢慢深入散列表(或者叫哈希表)这种数据结构。我们知道大多数数据结构在进行查询时的时间复杂度都不能达到常数级别，其原因在于我们在寻找代查找关键字前，总会遍历一些别的关键字。是否有种办法能解决这种开销从而能使我们无需遍历到别的关键字二直接访问到该元素呢？

​		回忆一下数组的访问方式，当我们访问数组索引为$i(i\ge0)$**下**的关键字时，我们只需要一步调用$array[i]$即可，时间复杂度达到了我们满意的$O(1)$。所以我们只要把关键字的大小作为索引，就能完美达到我们的要求(如果每个关键字还有对应的数据组成键-值对(字典)，那么索引下存储的是关键字对应的数据)。比如说我们关键字大小为11，那么我们直接访问$array[11]$即可。

### 1 直接寻址表的定义

​		我们把类似以上的数组形式称为==直接寻址表==。我们假设关键字取自于全域$U=\{0,1,{\dots},m-1\}$。那么我们建立数组(或直接寻址表)，为$T[m]$，其中的每一个位置我们称为==槽==$slot$ ，对应全域$U$中的一个关键字，但我们实际中不可能插入全域的所有元素，一般只使用到了其中的很小的一部分(假如我们只想存放集合{20,90,1000}中所有的关键字，为了访问1000，我们不得不建一个大小为1000的数组)，槽$k$(也就是$T[k]$)存放了**实际使用关键字**集合中关键字值为$k$的元素。 那些实际中未使用的关键字关键字所在的槽为空。具体如下图所展示的一样:

==示例:==

![SHashTable1](https://tva1.sinaimg.cn/large/e6c9d24egy1h0ie0qxj6ij20g409bjrw.jpg)

​		我们可以看到，直接寻址表可能存在空间上的很多浪费，因为大多数关键字所在的槽使用到。有些人可能会想如果查询的是字符串或者是负数怎么办，负数我们可以在其加一个整数让其变为正数即可，而字符串我们可以根据每个字符的ASCII码或者unicode码转换为整数，我们只需要在关键字上做特殊的处理就能存储。上述图示中我们的寻址表使用了额外的指针指向了数据，我们也可以直接将数据存储在数组中，

### 2 直接寻址表的操作

​		直接寻址表的操作非常简单，我们仅用伪代码描述。

#### 2.1 查找操作

​		查找我们直接将关键字作为数组索引即可。

==实现代码:==

```pseudocode
function FIND(T,key)
		return T[key]
```

==时间复杂度:==$O(1)$。

​			参数$T$为直接寻址表，$key$为关键字。

#### 2.2 插入操作

==实现代码:==

```pseudocode
function INSERT(T,key,value)
		T[key] = x
```

==时间复杂度:==$O(1)$。		

#### 2.3 删除操作

==实现代码:==

```pseudocode
function ERASE(T,key)
		T[key] = NULL
```

==时间复杂度:==$O(1)$。



## 三、散列表(Hash Table)

​		正如在第二节中所提到的，直接寻址表的缺点非常明显，当全域$U$非常大时，见一个这么大的表是非常不合适的，同时会产生非常多的空间浪费。

​		如何解决这种空间上的浪费是非常重要的问题，其实我们发现在直接寻址表中关键字和索引存在着紧密的联系，他们是一一对应的映射，所以有函数关系$h(key)=key$ ，我们通$h$将其关键字转换为对应的索引值，而根据直接寻址表我们发现了这个函数的转换并不好，会造成空间浪费，原因是表的大小取决于我们将要的关键字集合$K$中最大的关键字。我们需要寻找到一个函数$h$，它可以将一堆无规律的、混乱的(所以叫Hash，Hash的意思是混乱的)关键字集合转换至从0至$m-1$按顺序排列的关键字，其中$m-1$比之前全域$U$中的$m-1$要小的多，为$\theta(|K|)$(也就是说几乎和代使用关键字集合的大小是相等的)，因此$h(k)$(这种函数是存在的)得好好选择，我们将这种函数称为==散列函数==我们将在第三节中介绍几种好用的散列函数，我们将这种不通过直接映射转换，而是用特定的散列函数$h$进行关键字转换的表称之为==散列表(Hash Table)==。 	

### 1 散列表的定义

​		散列表将关键字为$key$的元素，通过散列函数$h(k)$，存放在槽$T[h(k)]$，我们将关键字$h(k)$生成的值称为==散列值==。为了展示Hash表的效果我们可以先看下图的示例:

==示例:==

![SHashTable2](https://tva1.sinaimg.cn/large/e6c9d24egy1h0ifw6yivrj20g608j74q.jpg)

​		从上面，我们可以看到原本需要建立长度为144的数组，在散列函数$h(k)=k\space\mathrm{mod\space}5$的转换下(对5取模，也就是除以5的余数)，这几个数只需要5个空间。并且仍然保持可时间复杂度为$O(1)$ ，(函数转换为$O(1)$)。

​		但我们可以注意到，散列表是可能会出现冲突(collison)的，我们将上面的143换一个数，比如65，那么这时候散列值就为0，他将和25在同一个槽中。我们这里可以换一个散列函数，但即使换一个散列函数，冲突可能仍然无法避免，因为关键字集合是不确定的。我们需要额外的措施解决这个问题。

### 2 链接法解决冲突

​		我们可以将经过散列函数散列到相同槽的关键字和数据组成一个链表来解决冲突问题。这样我们就能向下面图一样处理冲突。

==示例:==

​		                                 ![SHashTable3](https://tva1.sinaimg.cn/large/e6c9d24egy1h0iglw6x9nj20i408jt96.jpg)

​		链接法一定程度上会增加部分查找和插入的开销。

### 3 链接法散列表的操作

​		在正式进行之前，我们首先看下链表结点的定义:

==实现代码:==

```c++
struct Node//链表结点
{
    Node():next(nullptr),prev(nullptr){}
    Node(const int & k):key(k),next(nullptr),prev(nullptr){}
    Node(const int & k, Node* preNode, Node* nextNode):key(k),prev(preNode),next(nextNode){}
    int key;//关键字
    int value;//值
    Node* next;//之后的结点
    Node* pre;//之前的结点
}
```

​		我们定义的结点为双向链表的结点，为的是使删除函数更方便。

#### 3.1 查找操作

​		我们先获取关键字的散列值，然后找到对应槽的链表，进行搜索。

==实现代码:==

```c++
Node* HashTable::find(int key)
{
    Node* targetNode = _table[hash(key)];//获取该槽内链表的头结点
    while (targetNode != nullptr && targetNode->key != key)
    {
        targetNode = targetNode->next;
    }
    return targetNode;
}
```

​		其中$hash()$为散列函数。

#### 3.2 插入操作

​		插入操作我们仍然先获取待插入关键字的散列值，然后找到对应槽的链表，在链表的头结点处插入。

==实现代码:==

```c++
void HashTable::insert(int key, int value)
{
    int index = hash(key);
    //在链表的头结点插入
    Node* newNode = new Node(key, value , nullptr , _table[index]);
    _table[index] = newNode;//槽更改为存储新的头结点
}
```

#### 3.3 删除操作

​		删除操作我们先调用函数获取该结点，然后删除该结点。

==实现代码:==

```c++
void HashTable::erase(int key)
{
    Node* targetNode = find(key);//先找到该结点
    if (targetNode == nullptr)//如果该结点为空，直接返回
    {
        return ;
    }
    if (targetNode->pre != nullptr)
    {
        targetNode->pre->next = targetNode->next;//其前结点的next指针指向targetNode后一个结点        
    }
    else
    {
        _table[hash(key)] = targetNode->next;//更改新的头结点。
    }
    delete targetNode;
}
```

### 4 链接法散列表的性能分析

​		以上我们都没给出时间复杂度，我们将在这节中给出，其次这里将会证明在简单均匀散列的假设前提下，链接法散列函数的性能是良好的。

​		首先对于一个能存放$n$个关键字，拥有$m$个槽的散列表$T$，我们定义其$\alpha=n/m$为其==装载因子==，也讹我一个链的平均长度。后面我们将利用装载因子分析其性能。注意我们这里评价的是链接法散列表的平均性能。如果单纯只考虑其最糟糕的情况下，那么其时间复杂度为$\theta(n)$也就是所有关键字都散列到同一个槽中。但我们不能因为其最糟糕时的性能就去否定掉这个数据结构，因为在后面我们见会看到其平均性能非常优异。

​		现在我们假定每个关键字都等可能的散列到各个槽上，并且他们之间是相互独立的。我们成这个假设称为==简单均匀散列假设==。然后我们用$n_{j}$表示$T[j]$所对应的链表的长度。那么由于最多存储$n$个关键字，所以我们有:
$$
n=n_{0}+n_{1}+\dots+n_{m-1}
$$
​		而对于$n_{j}$，其期望值为$E[n_j{}]=\alpha=n/m$，这是因为期望本质上是在计算平均值，对于每个关键字等可能的落到各个槽上，那么一个槽的平均长度就是$n/m$,当然，也可用离散随机变量的期望的定义去计算，但是没这个必要。

​		假定我们计算$h(k)$的时间为$O(1)$。那么我们最终查找关键字$key$的时间取决于链表的长度$n_{h(key)}$ ，于是我们先确定算法查找元素的期望数，这的先分两种情况，因为一次查找可能是成功的，也可能是不成功的。

==定理1:==在简单均匀散列的假设下，在链接散列表中，一次不成功查找的平均时间为$\Theta(1+\alpha)$。

==证明:==一次不成功的查找意味着要扫描完整个链表。因为一个链表的长度的期望为$E[n_j{}]=\alpha$ , 再加上计算$h(key)$的时间$O(1)$，所以最终得到总时间$\Theta(1+\alpha)$。

==定理2:==在简单均匀散列的假设下，在链接散列表中，一次成功查找的平均时间为$\Theta(1+\alpha)$。

==证明:== 首先，成功的查找说明这个元素的在链表中。那么，要查找到这个关键字，我们需要考虑在插入这个关键字之后，插入到这个槽的关键字个数，这样考虑的原因是我们每次插入是在链表的头部插入，因此，后面插入的必然在链表的前面。而后面插入到这个槽中元素说明其关键字的散列值和我们带查找的关键字的散列值相同，所以我们要想得到在待查找关键字之后插入同一个槽中的关键字个数的期望(这将决定查找的平均时间复杂度)，我们首先得知道两个关键字落入同一个槽中的概率。我们设$x_{i}$为第$i$个插入槽中的结点($i\in[1,n]$) ，然后令$k_{i}=x_{i}.key$，之后我们设置随机变量$X_{ij}=I\{h(k_{i})=h(k_{j})\}$，也就是两个关键字落入同一个槽这个事件，根据简单均匀散列假设有$Pr\{{h(k_{i})=h(k_{j}})\}=1/m$，即两个关键字冲突的概率为1/m。由于给样本空间中只有这一个事件，所以$E[X_{ij}]=Pr\{{h(k_{i})=h(k_{j}})\}=1/m$，得到了冲突的期望之后，我们就可以计算对于每个插入结点$x_{i}$，	在其之后插入的结点与其冲突的期望数量。但是由于其期望数量还和插入结点$x_{i}$本身何时插入槽中有关(也即$i$的大小)。所以我们应该综合考虑所有结点的在其之后插入并与其冲突的期望数量，然后在总的冲突的期望数量求平均(除以n)，于是我们就能得到一次成功查找是期望的查找数目为:
$$
\begin{split}
E{\huge[}\frac{1}{n}\sum_{i=1}^{n}(1+\sum_{j=i+1}^{n}X_{ij}){\huge]}=& \frac{1}{n}\sum_{i=1}^{n}(1+\sum_{j=i+1}^{n}E{[}X_{ij}{]})&\qquad(期望的线性性质)\\ 
=&\frac{1}{n}\sum_{i=1}^{n}(1+\sum_{j=i+1}^{n}\frac{1}{m})=\frac{1}{n}\sum_{i=1}^{n}(1+\frac{n-i}{m})\\
=&1+\frac{1}{nm}\sum_{i=1}^{n}(n-i)=1+\frac{1}{nm}(n^{2}-\frac{n(n+1)}{2})&\qquad(等差数列求和)\\
=&1+\frac{n-1}{2m}=1+\frac{\alpha}{2}-\frac{\alpha}{2n}&(代入\alpha=\frac{n}{m})
\end{split}
$$


​		上面式中的1是因为待查找关键字本身也要算进去。因此，在加上计算$h(k)$的时间$O(1)$。因此总时间为$\Theta(2+\frac{\alpha}{2}-\frac{\alpha}{2n})=\Theta(1+\alpha)$。

​		经过上面的分析，我们可以推出如果$n$和$m$成正比。那么我们有$n=O(m)$，从而我们有$\alpha=n/m=O(m)/m=O(1)$。因此，**查找操作只需要常数时间，插入操作最多只需$O(1)$，删除操作也是常数时间。**

## 四、散列函数(Hash Function)

​		较好的散列函数能够给我们的散列表带来极大的帮助，包括避免冲突，并且能保证每个关键字都尽可能的均匀散列到每一个槽中，并与其他几个关键字在那个槽内无关。但在实际中，我们很难检验一个散列函数是否满足简单均匀散列假设，而且各个关键字之间相互独立这个条件也非常难满足。

### 1 除法散列法

​		除法散列法通过取$key$除以$m$的余数，将关键字$k$映射到$m$个槽中，其散列函数的定义如下:
$$
h(key)=key\space \mathrm {mod}\space m
$$
​		例如，假设我们有111个槽，那么132将会映射到槽$21$。

​		我们希望$m$最好是不太接近$2$的整数幂的**素数**，这样就会最有效的避免冲突。当然合数是我们最先排除在外的，因为其有约数，会产生更多的冲突，其次是排除$m=2^p$，也就是其为2的幂，除非已知最低$p$位的排列形式为等可能的。

### 2 乘法散列法

​		构造乘法散列法需要两个步骤，第一步需要用关键字$k$乘上常数$A(0<A<1)$，并提取$kA$的小数部分，第二步用$m$乘以这个值并向下取整。其散列函数的定义如下:
$$
h(key)=\lfloor m(keyA\space\mathrm {mod}\space 1)\rfloor
$$
​		乘法散列法的优点在于其对$m$的选择不是特别的关键，一般选其为$m=2^p$。这样计算机可以较快的处理这个散列函数。其原因是这样的，由于$m$是2的整数幂，所以最后的散列值我们可以取计算结果的$p$位(二进制表示)，我们在一个$w$位的计算机上一个$key$，假设可以用一个单字表示，那么其为$w$位，我们限制$A$为一个形如$s/2^w$一个分数，那么$key*A$。可以表示为$key*s/2^w$ ，那么$key*s$如下图所示:

==示例:==

![SHashTable4](https://tva1.sinaimg.cn/large/e6c9d24egy1h0iqybop13j208x065a9y.jpg)

​		我们只需要计算$s*k$就能取得结果了，$p$位散列值就在$r_2$的高$p$位。至于原因我们接着算下去就知道了。接下来我们需要除$2^w$，也就是将$r_1r_2$右移$w$位，如下:

==示例:==

![SHashTable5](https://tva1.sinaimg.cn/large/e6c9d24egy1h0ir4luijsj20br07g74a.jpg)

​		

​		我们可以看到$r_2$变成了小数位，之后我们对1求余数，只剩下小数位，如下:

==示例:==

![SHashTable6](https://tva1.sinaimg.cn/large/e6c9d24egy1h0ir7gteqqj20br094glo.jpg)

​		

​		之后，由于我们的m是$2^p$,相乘相当于左移$p$位。如下:

==示例:==

![SHashTable7](https://tva1.sinaimg.cn/large/e6c9d24egy1h0ira96tu9j20br0bqdfy.jpg)

​		

​		我们发现左移$p$位后本质上就是取$r_2$的高$p$位，所以我们可以在计算完$s*key$之后直接取$r_2$的高$p$位即可。

​		要想避免冲突，我们需对$A$做一些考虑，这里我们直接给出结论，大名鼎鼎的《计算机程序设计的艺术》的作者$Knuth$给出的较为理想$A$值。
$$
A\approx(\sqrt{5}-1)/2
$$
​		最后一种散列函数我们将介绍全域散列法，但这个方法我们将在下一节中介绍。

## 五、全域散列法(universal Hashing)(拓展)

​		我们前面的散列法的时间性能大多是基于简单均匀散列的假设前提下，但是如果遇到一个恶意的对手来针对某个特定的散列函数来选择特定的关键字，使其都落入一个槽中，那么链接法的散列表将会进入最糟糕的状态，每次检索将会使得平均的检索时间为$\Theta(n)$。任何一个散列函数只要经过适当的针对都会出现拥塞的情况，比如说$h(key)=key\space\mathrm{mod}\space 11$，我们如果所有关键字取11的倍数那么将会使得所有关键字都落入0这个槽内。

​		针对以上的情况，我们应该随机的选择散列函数，使之==独立于==要存储的关键字。也就是不管插入什么关键字，都不会影响散列的平均性能。我们把这种方法称之为==全域散列法==。由于是随机化选择散列函数，所以对于那些针对单个散列函数得插入序列，将不再会导致最坏情况，并提供较好的平均性能。当然如果恰还随机到一个散列性能较差的函数，那么还是会出现较差的性能。因此我们会对随机选择的所有散列函数加以一些限制，使其最终能达到我们之前假定的在简单均匀散列假设的条件下，链接散列表的平均性能。

### 1 全域散列法的定义

==定义:==设$\mathcal H$ 为一组有限的散列函数，它将给定的关键字全域$U$映射到$\{0,1,2,\dots,m-1\}$中，如果对每一对不同的关键字$k,l\in U$，满足$h(k)=h(l)$的散列函数的个数不超过$\mathcal {|H|}/m$ ($\mathcal{|H|}$代表散列函数的个数)。那我们说这样的一个函数组是==全域==的。会这样定义原因在于$\mathcal {|H|}/m$ 限制了对于随机选择==单个==散列函数，当关键字$k\neq l$，那么$h(k)=h(l)$的概率不超过$1/m$，回忆一下前面讲过的，在简单均匀散列的假设下，我们能确定每个关键字发生冲突的概率为$1/m$，然后我们验证了在$1/m$的限制下链接法散列表的优越的平均性能。现在使用全域散列，我们确保了其发生冲突关键字的概率最多为$1/m$，因此我们就可以保证全域散列法能达到非常优越的平均性能。

==定理3:==如果$h$选自一组全域散列函数，将$n$个关键字散列到用链接法解决冲突并含有$m$个槽的散列表中。如果$k$不在表中，那么其将要被散列至的槽的链表的期望长度$E[n_{h(k)}]$至多为$\alpha=n/m$，如果$k$在表中，那么其所在的槽的链表的期望长度$E[n_{h(k)}]$至多为$\alpha+1$(包含了关键字$k$本身)。

==证明:==链表的长度和发生冲突有关，我们定义随机变量$X_{kl}=I\{h(k)=h(l)\}$，由全域散列函数的定义，我们知道每一对关键字发生冲突的概率不超过$1/m$ ，所以我们有$Pr\{h(k)=h(l)\}\le1/m$，所以我们有$E[X_{kl}]\le1/m$ ，因为唯一的事件$I\{h(k)=h(l)\}$要么发生要么不发生。$E[X_{kl}]=1*Pr\{h(k)=h(l)\}+0*Pr\{h(k)\neq h(l)\}\le1/m$ 。

​		下面对每个关键字$k$，定义随机变量$Y_{k}$ ，表示与$k$散列到同一槽位中非$k$的关键字个数。于是我们有:
$$
Y_{k}=\sum_{l\in T \space ,l\neq k }X_{kl}
$$
​		从而，有:
$$
\begin{split}
E[Y_{k}]=&E{\large[}\sum_{l\in T,l\neq k}X_{kl}{\large]}=\sum_{l\in T,l\neq k}E[X_{kl}]\\
\le&\sum_{l\in T,l\neq k}\frac{1}{m}
\end{split}
$$
​		下面分情况讨论，假设$k$不在表中，那么$n_{h(k)}$=$Y_{k}$。并且$|\{l:l\in T且l\neq k\}|=n$。(这是因为$k$不在表中，所以表中任意一个关键字和$k$都不相等)，所以$E[Y_{k}]=n/m=\alpha=E[n_{h(k)]}]$。假如$k$在表中，那么$n_{h(k)}=Y_{k}+1$(因为还要包含$k$本身)。并且$|\{l:l\in T且l\neq k\}|=n-1$（因为$k$在表中），所有$E[Y_{k}+1]=(n-1)/m+1=\alpha+1-1/m<1+\alpha$。因此我们可以得到$E[n_{h(k)}]<1+\alpha$。得证。

==推论1:==对于一个具有$m$个槽位且初始时为空的表，利用全域散列法和链接法解决冲突，需要$\Theta(n)$的期望时间来处理包含了$n$个$insert$、$find$和$erase$的操作序列，该操作序列包含了$O(m)$个$insert$操作。

==证明:==由于插入数为$O(m)$,因此有$n=O(m)$，从而有$\alpha=O(1)$，$Insert$需要常量的时间，由之前的定理2我们可以知道每一个$Find$操作需要$O(1)$，所以最后$erase$操作也只需要$O(1)$，因此对于$n$个这样的操作序列，我们最终只需要$\Theta(n)$的时间。

### 2 设计一个全域散列函数类

​		要理解这里，需要一些数论的知识，包括求解模的线性方程。

​		首先我们选取一个足够大的素数$p$，使得每一个关键字$k$落到$[0,p-1]$的区间上，这么做的原因在于使用$\mathrm{mod}\space p$模操作时不会产生然和冲突(当然我们不会建这么大的表)。设$Z_{p}=\{0,1,\dots,p-1\}$ ,以及$Z_{p}^*=\{1,2,\dots,p-1\}$ 。这两个定义分别是模$p$的加法群和模$p$的乘法群，详情请见数论。

​		现在假定我们的槽数为$m$，对于任何$a\in Z_{p}^*$，以及$ b \in Z_{p}$ ，定义散列函数$h_{ab}$。利用一次线性变换，再进行模$p$和模$n$的归约。有:
$$
h_{ab}(k)=((ak+b)\space\mathrm{mod}\space p) \space\mathrm {mod}\space m
$$
​		由于这个散列函数的$a$有$p-1$种选择，$b$有$p$种选择，所以总共有$p(p-1)$个函数，这些函数组成了一个函数集合为:
$$
\mathcal{H}_{pm}=\{{h_{ab}:a\in[1,p-1]},b\in[0.p-1]\}
$$
​		下面我们将推断以上定义的函数集合$\mathcal {H}_{pm}$是全域的。

==定理4:==以上两个式子定义的$\mathcal {H}_{pm}$是全域的。

==证明:==首先由内向外，先看$(ak+b)\space\mathrm{mod}\space p$。假设首先在全域$U$中存在两个不同的关键字$k$和$l$，对于一个给定的散列函数($a$和$b$已经确定)。我们有:
$$
r=(ak+b)\space\mathrm{mod}\space p\\
s=(al+b)\space\mathrm{mod}\space p
$$
​		我们可以首先确认的是$r\neq s$，我们可以用两者相减是否为0来判断，两式子相减得到$r-s\equiv (a(k-l))\space\mathrm{(mod}\space p)$(模减法，还有就是你得注意到$r\equiv(ak+b)(\mathrm{mod}\space p)$，这是因为$r<p$)，由于$a\in Z_p^{*}$ ，并且$k$和$l$都是落在$[0,p-1]$区间上的关键字，$|k-l|\in[0,p-1]$。$\gcd(a,p)=1$，$\gcd(k-l,p)=1$,我们得到$\gcd{(a(k-l),p)}=1$。所以两者互质，$a(k-l)$必然不能被$p$整除。所以$r\neq s$ 。这说明了第一轮取模不会产生冲突。

​		然后我们注意我们的的函数集合根据$a$和$b$中不同取值的组合总共有$p(p-1)$个函数，我们可以得到每一对$(a,b)$在上述方程组中能够产生与其对应的($r,s$)，并且**不同的$(a,b)$产生$(r,s)$对互不相同**。要证明这个，我们考虑给定$r$和$s$，以及$k$和$l$。根据$r-s\equiv(a(k-l))\space\mathrm{(mod}\space p)$我们可以根据求解模的线性方程求出$a$ ，并且由于$\gcd(k-l,p)=1$，我们知道这个解是唯一的(任意$n>1$，如果$\gcd(a,n)=1$，那么模线性方程$ax\equiv b(\mathrm{mod}\space n)的解唯一$，同理在解出来$a$之后，我们就可以得到$r-ak\equiv b(\mathrm{mod}\space p)$ ，我们同样有$\gcd(1,p)=1$，所以我们知道了解出来的$b$也是唯一的。也就是说每一对$(r,s)$对应唯一一对$(a,b)$。

​		之后，我们给定$(a,b)$以及$k$和$l$，对于每一对$(a,b)$和上面同样的方式(还是上面那个方程，并且都有$\gcd(1,p)=1$)，我们能得到每一对$(a,b)$都对应唯一一对$(r,s)$，最后，综合两者我们可以得到两者是$(a,b)$和$(r,s)$是一对一映射(结论可以参考离散数学函数那一章)，所以任何给定的输入对$k$和$l$，如果从$Z_{p}^*\times Z_{p}$（笛卡尔积）中均匀的选择$(a,b)$，则结果数对$(r,s)$就等可能地为任何不同的数值对。

​		解决了上述问题后，由于每个函数都产生不同的$(r,s)$对，所以最后关键字发生冲突的概率只和$r\equiv s(\mathrm{mod}\space m)$的概率有关。如果$r$给定，那么$s$就只能取剩下的$p-1$个值($s\neq r$)，所以我们知道最多存在:
$$
\begin{split}
\lceil\frac{p}{m}\rceil-1\le&((p+m-1)/m)-1\\
&=(p-1)/m
\end{split}
$$
​		个$s$的值使得$r\equiv s(\mathrm{mod}\space m)$。要理解$\lceil\frac{p}{m}\rceil-1$你得注意到如果两个数对同一个数取模相等，那么这两个数一定是其取模后的值(假设为$i$)加上$j*m,j\in \Z$，即$(i+jm)$。所以最后我们得到$s$与$r$关于模$m$进行归约时，$s$和$r$发生冲突的概率至多为$((p-1)/m)/(p-1)=1/m$。所以对于任何不同的数对$k,l\in Z_{p}$，有
$$
Pr\{h_{ab}(k)=h_{ab}\}\le1/m
$$
​		所以我们知道$\mathcal H _{pm}$是全域的。

## 六、开放寻址法(open addressing)	

### 1 开放寻址法的定义

​		开放寻址法是相对于链接法的散列表的另一种形式，它最大的特点在于所有元素都存直接存放在散列表中。完全不使用链表。所以该方法会导致当表填满后，就无法再插入新的元素，所以其装载因子$a\le1$。这种方法相对于链接法的散列表节省了很多空间。

​		除此之外，开放寻址法的插入和查找很有特点，考虑这样一个情景，当我们插入一个关键字时，由于我们的散列函数会发生冲突的原因。该槽已经被另一个关键字所占用了，而我们又没有链表去解决这些问题，因为我们的关键字及其对应的数据是直接存放在散列表的槽中的。应该如何解决这个问题使得操作成功。

​		解决方案是我们需要在向后知道找到一个空的槽，这个槽不能乱找，**而是要依赖于关键字的值**，我们把这个寻找的过程称之为==探查==，为了使其后面要插入的空槽依赖于关键字，我们仍然需要依靠我们的散列函数，于是现在扩充我们散列函数的定义，我们使其接受额外一个参数，称为探查数。我们刚开始插入时，直接调用$h(k,0)$，如果这个槽被占用了，我们就调用$h(k,1)\dots$，直到$h(k,m-1)$(其中$k$为关键字，$m$为槽数)，如果调用$h(k,m-1)$还是不行，说明散列表已经没有空槽插入新元素，拒绝本次插入。

​		上述过程会产生一个==探查序列==:
$$
<h(k,0),k(k,1)\dots h(k,m-1)>
$$
​		对于查找，也是类似和插入一样的过程。一般开放寻址法的散列表不支持删除，具体原因我们在介绍完其操作后给出。

### 2 使用开放寻址法散列表的操作

#### 2.1 插入操作

==实现代码:==

```c++
void psHashTable::insert(int k)
{
    int i = 0 ;
    for (; i < _tableSize; i++)
    {
        if(_table[hash(k,i)].second == true)
        {
            _table[hash(k,i)] = {k, false};//找到空槽插入元素
            return;
        }
    }
    throw std::runtime_error("HashTable overflow!");
}
```

​		在代码中我们的$\_table$采用的形式时std::vector< < std::pair<int, bool>>，其中int为关键字类型，bool用来判断槽是否为空，为了简单起见，我们只存储了关键字，并假定其并没有对应数据。$hash$的具体实现形式将在这一节中的探查序列那一节介绍。

#### 2.2 查找元素

==实现代码:==

```c++
std::pair<int, bool> psHashTable::find(int k)
{
    int i = 0;
    do
    {
        auto temp = _table[hash(k,i)];
        if (temp.first == k)
        {
            return {k, false};
        }
        i++;
    }
    while(temp.second == true && i != _tableSize)
    return {-1, true};
}
```

​		现在我们回过头来看下为什么开放寻址法的散列表一般不支持删除操作。考虑以下情景，我们在插入一个关键字$k$时，发现槽被占用，占用关键字为$i$，于是我们在后面找到了一个空槽插入。然后在插入之后，原本占用槽的$i$在之后被删除了，如果我们这时候再次调用$find(k)$，那么根据算法，我们将不会找到$k$。因为删除后槽为空，而我们的查找算法遇到空槽就应该结束。所以在后面的$k$就搜寻不到了，解决方法还是有的，我们需要对每个槽增加额外的状态$deleted$，如果一个关键字被删除，我们将槽置为$deleted$而非置为$NULL$。但如果这么做，我们的查找的时间复杂度就不依赖于装载因子$\alpha$，因此对于有删除的需求，我们一般采用链接法的散列表去解决。

### 3 探查序列

​		我们现在确定开放寻址法中的均匀散列假设:**每个关键字的探查序列等可能的为$<0,1\dots,m-1>$的$m!$种排列的任意一种**。

​		但在实际中对于每个关键字$k$满足实现$m!$个的探查序列几乎不存在。以下将介绍下面三种技术，他们之中最多能产生的探查序列数位$m^2$。

#### 3.1 线性探查

​		我们采用一个一个辅助的散列函数$h'$ ，他能将关键字散列到$[0,m-1]$的槽中。线性探查采用的散列函数如下。
$$
h(k,i)=(h'(k)+i)\space\mathrm{mod}\space m,\qquad i=0,1\dots,m-1
$$
​		这个方法本质上就是在遇到被占用的槽时，顺着检查其下一个槽是否为空，如果当前槽正好是最后一个槽，那就绕回第一个槽检查其是否为空。具体如下图:

==示例:==

![SHashTable8](https://tva1.sinaimg.cn/large/e6c9d24egy1h0jw0u0d8gj207o07d748.jpg)

​		

​		前两个槽发现都被占用，知道第三个槽才找到空位插入。

​		但是这个方法会造成==一次集群==问题，随着连续被占用的槽不断增加，我们使用查找函数的效率不断降低，这是因为当一个空槽前有$i$个满槽时，这个空槽为下一个被占用的概率为$(i+1)/m$，所以随着插入的增多，连续被占用的槽的数量会越来越大。并且每个关键字最多只能产生$m$种序列，只和其开始位置有关。

#### 3.2 二次探查

​		二次探查采用如下形式的散列函数:
$$
h(k,i)=(h'(k)+c_{1}i+c_{2}i^2)\space\mathrm{mod}\space m
$$
​		其中$c_{1},c_{2}$为常数，其中$c_{1},c_{2}$和$m$的取值要受到限制。这种方法比线性探查的情况会稍微好一点，因为其后加的偏移量不是线性的，但其仍然会产生==二次集群==问题。并且关键字$k$所能产生的序列也只和其初始位置有关，因此最多产生$m$种序列。

#### 3.3 双重散列

​		==双重散列==是用于开放寻址法最好的方法之一，它所产生的排列具有随机选择排列的很多特性。双重散了采用以下形式的散列函数:
$$
h(k,i)=(h_{1}(k)+ih_{2}(k))\space \mathrm{mod}\space m
$$
​		 初始探查位置为$T[h_{1}(k)]$，后续的探查位置为食前一个位置加上偏移量$h_{2}(k)$模$m$,这里探查序列以==两种==不同方式依赖于关键字$k$，包括其初始位置还有其偏移量。前面两种技巧主要以初始位置依赖于$k$，因此只要初始位置定了，那么关于这个关键字$k$的序列也就定下来了。而双重散列不是这样的，因此可以预测双重散列能产生更多种序列，最接近于开放寻址法中的均匀散列假设。

​		这里面需要注意的是，值$h_{2}(k)$必须要与表大小$m$互素。较为简单的实现是$m$取为2的幂，并设计一个总产生奇数的$h_{2}(k)$ 。另一种方法是取$m$为素数，并设计一个总时返回较$m$小的散列函数$h_{2}(k)$。

​		例如，我们可以设计如下的散列函数:
$$
h_1(k)=k\space\mathrm{mod}\space m,\space h_2(k)=1+(k\space\mathrm{mod}\space m')
$$
​		现在我们假设$m$为13，$m'=11$，插入关键字14，如下图所示:

==示例:==

![SHashTable9](https://tva1.sinaimg.cn/large/e6c9d24egy1h0kofyvhv4j203u0c6aa1.jpg)

​		

​		上面的示例中，首先计算$1\equiv14\pmod{13}$ ，查找到槽1，结果槽1倍79占用，令$i=1$，$h_2(k)=1+(14\space\mathrm{mod}\space 11)=4$ 。$5\equiv(4+1)\pmod{13}$。查找到槽$5$，槽5也被占用，最后令$i=2$，计算$h_2(k)$ ，最后落到槽9 。

​		当$m$为素数或为2的幂时，双重散列法中用到了$\Theta(m^2)$种探查序列。相较于前两种技术较为理想。

### 4 性能分析

​		我们仍然以装载因子$\alpha$为基础来分析其性能。且我们假设在均匀散列性质的前提下，也就是每一种序列都是等可能的。

==定理5:==在简单均匀散列的前提下，给定一个装载因子为$\alpha=n/m<1$的开放寻址散列表， 一次不成功的查找至多需要$1/(1-\alpha)$次。

==证明:==一次不成功的查找，最后查找到一定是一个空槽，而前面可能会遇到被占用的槽，我们定义随机变量$X$为一次不成功查找所需要遍历槽的次数。再定义事件$A$为第$i,(i={1,2\dots})$次遇到被占用的槽，那么如果我们至少在第$i$次才遇到空槽，那么事件${X\ge i}$即为事件$A_1\cap A_2\cap\dots\cap A_{i-1}$ 。同时对于任意事件集，我们有:
$$
Pr\{A_1\cap A_2\cap\dots\cap A_{i-1}\}=Pr\{A_1\}*Pr\{A_2|A_1\}*Pr\{A_3|A_1\cap A_2\}\dots Pr\{A_{i-1}|A_1\cap A_2\cap\dots\cap A_{i-2}\}
$$
​		然后我们注意到$Pr\{A_1\}=n/m$，之后假设为第$j(j>1)$个占用槽，$Pr\{A_j|A_1\cap A_2\cap\dots\cap A_{j-1}\}=(n-j+1)/(m-j+1)$。这是因为总共$m$个中已经有$j-1$个是被占用的了，并且$n$个关键字中有$j-1$个被使用了，那么根据简单均匀散列假设，剩下的关键字应该在剩下的槽中均匀分配。于是我们有:
$$
Pr\{X\ge i\}=\frac{n}{m}\cdot \frac{n-1}{m-1}\cdot\dots\cdot\frac{n-i+2}{m-i+2}\le(\frac{n}{m})^{i-1}=\alpha^{i-1}
$$
​		然后我们得出探查期望数的界:
$$
E[X]=\sum_{i=1}^{\infty}Pr\{X\ge i\}\le\sum_{i=1}^{\infty}\alpha^i=\frac{1}{1-\alpha}
$$
​		上述公式在$X$取自然数时成立。如果$\alpha$是一个常数，那么我们最后所需的运行时间为$O(1)$，同样这取决于开放寻址散列表的填充程度，其越满，所需的探查次数越多。例如90%满时，平均探查次数为10次。

==推论2:==假设采用均匀散列假设，平均情况下，向一个装载因子为$\alpha$ 的开放寻址散列表中插入一个关键字至多需要$1/(1-\alpha)$次探查。

==证明:==这个其实非常好证明，因为，其和一次不成功的插入的情况是一样的，我们需要找到第一个空槽。

​		对于一次成功的查找，需要做一些额外的工作来得到期探查次数。

==定理6:==在均匀散列的假设下，对于一个装载因子$\alpha<1$的开放寻址散列表。一次成功的查找中的期望探查次数为至多为:
$$
\frac{1}{\alpha}\ln\frac{1}{1-\alpha}
$$
==证明:==其实我们查找到关键字$k$所需要遍历的探查序列，和插入关键字$k$所需遍历的探查序列是一样的。因为调用的探查函数并没有发生变化。所以如果$k$是第$i+1$个插入的关键字，那么根据推论2，我们有查找到$k$所需要的探查次数为$1/(1-i/m)=m/(m-i)$。这是其中某一个关键字的探查次数，我们需要获取每个关键字探查次数的平均值，因此我们应该对每个关键字的探查次数求和然后再取平均求得其平均探查次数，如下:
$$
\begin{split}
\frac{1}{n}\sum_{i=0}^{n-1}\frac{m}{m-i}=\frac{m}{n}\sum_{i=0}^{n-1}\frac{1}{m-i}=&\frac{1}{\alpha}\sum_{k=m-n+1}^{m}\frac{1}{k}(令k=m-i)\\
\le&\frac{1}{\alpha}\int_{m-n}^{m}\frac{1}{x}dx\qquad(这个可以由几何意义得到)\\
=&\frac{1}{\alpha}\ln\frac{m}{m-n}=\frac{1}{\alpha}\ln\frac{1}{1-\alpha}
\end{split}
$$
​		综上，得证。

## 七、完美哈希(perfect Hashing)(拓展)

​		我们介绍的散列表的关键字集合是==静态==，原因在于我们的关键字集合一旦确定下来后面就不会在变化了，还有一些动态关键字的散列表将在高阶数据结构中介绍，在静态关键字集合下，散列表能提供非常优异的平均性能，接下来要介绍的完美哈希(也叫完全散列)在静态关键字集合的这个条件下，用该方法进行查找时，能在最坏情况下用$O(1)$次完成访问。它用到了前面全域散列法的知识。

### 1 完美哈希的定义

​		原有的散列表都为一级散列表，完美哈希要求采用二级散列表，并且每级散列表使用前面提到的全域散列。它的形式类似如下:

==示例:==

![SHashTable10](https://tva1.sinaimg.cn/large/e6c9d24egy1h0kusk7rrxj20pn0cxq3u.jpg)

​		首先由一级散列函数$h(k)=((ak+b)\space\mathrm{mod}\space p)\space\mathrm{mod}\space m$将关键字在一级散列表$T$中分配到对应的槽。然后我们为每个槽单独制定一个全域散列函数$h(k)=((a_ik+b_i)\space\mathrm{mod}\space p)\space\mathrm{mod}\space m_i$ ，其中$i\in[0,m-1]$。将在第一轮散列到这个槽中的关键字个数$n_{i}$ 再次散列到每个槽的单独的散列表$S_i$。这样就不在需要链表了。为了在第二次散列后不发生冲突，我们$m_i$的取值应该为$n_i$的平方。后面会说明为什么会这么取。在二级散列表不发生冲突的情况下，查找的最坏情况也只需要$O(1)$的时间复杂度。

​		上述需要注意的是，如果$n_i=m_i=1$，那么我们可以直接取$a_i=b_i=0$，不用给他专门制定一个散列函数。

​		其次这种存储方式可能会导致空间上的大量开销，因为$n_i^2=m_i$，但我们后面将会证明，通过精心挑选第一级散列函数，我们能将其空间复杂度保持在$O(n)$。

### 2 冲突分析和空间开销分析

==定理7:==如果从一个全域散列函数类中随机选出散列函数$h$，将$n$个关键字存储在一个大小为$m=n^2$的散列表中。那么表中出现冲突的概率小于$1/2$。

==证明:==总共有$\binom{n}{2}$对关键字(组合数)会发生冲突，如果散列函数是从$\mathcal H$中随机选出的，那么根据全域散列函数的定义，每一对关键字发生冲突的概率不超过$1/m$ 。设$X$为一个统计冲突次数的随机变量。那么当$m=n^2$时，期望的冲突次数为:
$$
E[X]=\binom{n}{2}\cdot\frac{1}{n^2}=\frac{n^2-n}{2}\cdot\frac{1}{n^2}<\frac{1}{2}
$$
​		上述不等式中我们再计算$E[X]$中发生冲突$x=1$，反之为0，我们忽略了和0相乘的项，因为其不会影响结果。之后我们再代入马尔可夫不等式$Pr\{X\ge t\}\le E[X]/t$，其中$t=1$，这代表至少发生一次冲突。代入后我们就完成了证明。

​		因此我们知道了从$\mathcal H$中速记选择函数发生冲突的概率不超过1/2，因此我们可以对==静态==的关键字集合$K$做几次随机的尝试，就能选择出一个没有冲突的散列函数。当然，如果直接在一级散列表上去$m=n^2$（也就是不使用二级散列表），那么我们的空间复杂度就是$O(n^2)$，这显然不是我们所期望的，所以我们先用一级散列表分散到各个槽中，在对每个槽中的关键字$n_i$取平方，下面我们将证明，在一级散列函数选取得当的情况下，空间复杂度为$O(n)$。

==定理8:==如果从某一个全域散列函数类中随机地选出散列函数$h$，用它将$n$个关键字存储到一个大小为$m=n$的则有:
$$
\space\space\space\space\space\space E{\large[}\sum_{i=0}^{m-1}{n_i^2}{\large]}<2n\qquad\qquad\qquad\qquad\qquad\qquad
$$
​		这里$n_i$为散列到槽$i$中的关键字个数。

==证明:==首先我们知道对于一个非负整数$a$，下面这个等式恒成立
$$
\qquad\qquad\qquad a^2=a+2\binom{a}{2}\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\tag{1.1}
$$
​		利用这个式子，我们开始进行推导:
$$
\begin{split}
\qquad\qquad\qquad \qquad \qquad 
\qquad \space\space\space E{\large[}\sum_{i=0}^{m-1}{n_i^2}{\large]}=&E{\large[}\sum_{i=0}^{m-1}{n_i}+2\binom{n_i}{2}{\large]}&\qquad&(由式1.1)\\
=&E{\large[}\sum_{i=0}^{m-1}{n_i}{\large]}+2E{\large[}\sum_{i=0}^{m-1}\binom{n_i}{2}{\large]}&\qquad&(期望的线性性质)\\
=&E[n]+2E{\large[}\sum_{i=0}^{m-1}\binom{n_i}{2}{\large]}&\qquad&(所有槽中分配关键字之和为n)\\
=&n+2E{\large[}\sum_{i=0}^{m-1}\binom{n_i}{2}{\large]}&\qquad&(n不是一个随机变量)
\end{split}
$$
​		下面我们要计算和式$\displaystyle\sum_{i=0}^{m-1}\binom{n_i}{2}$ ，其正好是散列表中发生冲突的关键字总对数。由于我们选择的是全域散列函数，根据全域散列函数的性质，这一和式的期望值至多为:
$$
\binom{n}{2}\frac{1}{m}=\frac{n(n-1)}{2m}=\frac{n-1}{2}\qquad\qquad\qquad
$$
​		上式中$\binom{n}{2}$为全域散列函数中函数的总个数，即为$|\mathcal H|$，因为$m=n$，于是:
$$
E{\large[}\sum_{i=0}^{m-1}{n_i^2}{\large]}\le n+2\frac{n-1}{2}=2n-1<2n
$$
​		于是得证，同时我们可以得到这种情况下的空间复杂度为$O(n)$。

==推论3:==如果从某一全域散列函数类中随机选出散列函数$h$，用它将$n$个关键字存储到一个大小为$m=n$的散列表中，并将每个二次散列表的大小设置为$m_i=n_i^2(i=0,1,2,\dots ,m-1)$ ，则在一个完全散列方案中，存储所有二次散列表所需的存储总量的期望值小于$2n$。

==证明:==因为$m_i=n_i^2$，由定理8，我们可以知道:
$$
E{\large[}\sum_{i=0}^{m-1}m_i{\large]}=E{\large[}\sum_{i=0}^{m-1}{n_i^2}{\large]}<2n\qquad\qquad
$$
​		证明完毕。

==推论4:== 如果从某一全域散列函数类中随机选出散列函数$h$，用它将$n$个关键字存储到一个大小为$m=n$的散列表中，并将每个二次散列表的大小设置为$m_i=n_i^2(i=0,1,2,\dots ,m-1)$ ，则用于存储所有二级散列表的存储总量等于或者大于4n的概率小于1/2。

==证明:==我们直接运用马尔可夫不等式，$Pr\{X\ge t\}\le E[X]/t$，代入$X=\displaystyle\sum_{i=0}^{m-1}m_i$，和$t=4n$代入不等式，我们可以得到:
$$
\qquad\qquad Pr\{\sum_{i=0}^{m-1}m_i\ge 4n\}\le E[\sum_{i=0}^{m-1}m_i]/4n<\frac{2n}{4n}=\frac{1}{2}
$$
​		因此我们可以得到我们只需要从全域函数类中随机选择一个散列函数，只要尝试几次就能达到我嗯想要的空间复杂度$O(n)$

## 八、$Rabin-Karp\space Algorithm$和滚动哈希(Rolling Hashing)

​		在这一节中，我们将介绍滚动哈希以及其重要的一个应用$Rabin-Karp\space Algorithm$,一个字符串匹配算法。

​		我们首先考虑一个情景，假设我们有一个字符串$S$，然后我们有一个带匹配的字符串$P$，现在我们需要确定$P$是否在$S$中，也就是说我们想要确认$P$是否为$S$的子字符串。假设字符串$S$的长度为$n$，字符串$P$的长度为$l$。

### 	1 暴力算法

​		首先最简单无脑的就是暴力算法，我们从字符串$S$第一个元素开始取长度为$l$的字符串，然后在这长度为$l$的子串上一一比对。如果这长度为$l$的子串中每个字符都和$P$中的相等，那么我们己找到了一个匹配，然后我们移动到$S$第二个位置。继续重复上述的步骤进行比对。直到遍历到字符串S的$n-l+1$处。

==实现代码:==

```c++
#include <string>
int subStringMatching(const std::string & S,const std::string & P)//字符串匹配，最后返回匹配个数
{
  	int result = 0;//最后的返回值，返回匹配个数
  	int n = S.size();//获取长度
  	int l = P.size();
  	for(int i = 0;i < n - l + 1; i++)
    {
      	int j = 0;
    		for(; j < l; j++)
        {
          	if(S[i + j] != P[j])
            {
              	break;
            }
        }
      	if(j == l)
        {
          	result++;
        }
    }
  	return result;
}
  
```

==时间复杂度:==$O(ln)$。我们每次为了验证是否$S$的子串是否和$P$相等，需要$O(l)$，而这个动作我们需要持续$O(n-l)=O(n)$次，因此最后时间复杂度为$O(nl)$。

​		上面的这个时间复杂度显然是不理想的，我们考察一下为什么会需要这么长时间，其实我们稍微仔细的考察一下，我们可以发现我们每次检验$S$的子字符串是否和$P$匹配时，我们做了许多多余的操作。具体如下图所示:

==示例:==

![SHashTable11](https://tva1.sinaimg.cn/large/e6c9d24egy1h0l2w20jzqj20cu0e6jru.jpg)

​		我们可以看到我们前两次扫描$S$的子字符串中有交叠的部分，在上图中的为$bc$，其实我们在第一次扫描中已经知道了$S[2]$和$S[3]$，但在第二次扫描中我们依然还要去获取它，这种扫描是冗余的。我们得想办法去除这种扫描中的冗余，这样我们的匹配算法就能大大增快我们只需要第一次的$O(l)$单次扫描加上后面我们每次只要额外扫描一个字符，总共有$O(n-m+1)$次，我们能达到几乎线性的时间复杂度。

​		但可惜的是我们如果用字符比较是无法解决这种冗余的，但我们不妨换个思路，字符不行，但是字符可以转换成数字(ASCII)。这是我们将要在下一节中讲的。

### 2 数字字符串匹配

​		简单起见，现在我们假设我们的字符串中的字符只由${0,1,2,3,4,5,6,7,8,9}$的字符组成。本质上我们也可以通过将字符串中每个字符对应的字符码转换成数字。我现在看下我们是否有办法解决上面提到的冗余以达到我们期望的时间复杂度。

​		对于这种只有数字组成的字符串我们可以这样确定连个字符串是否相等:我们把每个字符串的代表的十进制整数算出来，如果两个字符串最后算出来的10进制整数是相等的，那我们就说这两个字符串是匹配的。对于如何计算字符串代表的十进制整数值，我们可以按如下的多项式去计算，我们设基数为$b$，在我们的例子中$b=10$为十进制，待计算字符串为$P$，长度为$m$，最后结果为$r$，计算式如下:
$$
r=P[m-1]+b^1P[m-2]+b^2P[m-3]+\dots+b^{m-1}P[0]\qquad\qquad
$$
​		注意我们的字符串最开始的位$P[0]$为最高位。

​		但这个计算过程不够快，它有冗余，原因是有多个$b^i$被重复计算多次，我们可以运用==霍纳法则==使得这个计算式的时间复杂度控制在$\Theta(m)$。
$$
\begin{equation}\qquad\qquad \qquad \qquad r=P[m-1]+b(P[m-2]+b(P[m-3]+\dots+b(P[1]+bP[0])\dots))\tag{1}\qquad\qquad\qquad\qquad\
\end{equation}
$$
​		接下来我们可以看到使用数字可以去除在暴力算法中的冗余，首先举个例子，假设待匹配字符$P$为$234$，而字符串$S$为较为简单的$1234$。我们最开始执行算法时，首先利用式(1)计算好带匹配字符串$P$的所代表的十进制数字$d_{p}$，然后开始第一轮迭代，计算出字符串$S$的前三位字符组成的子字符串代表的十进制数字$r_0$。我们通过比对两者确定其是否相等来判断其是否匹配。然后我们进入第二轮迭代，提取并计算字符串$S$  2～4位的子字符串$234$，但我们无需再向上面那样计算了，我们可以利用之前第一轮的计算结果$r_{0}$，将其最高位(为1)丢掉，然后其余的乘基数$10$,最后加上新加入的$4$就能快速计算出$r_{1}$了。我们可以看到，这个算法会相对于之前的算法更有效率。下面我们给出由已知结果$r_i$来计算$r_{i+1}$的公式:
$$
\space\space\qquad\qquad\qquad\qquad r_{i+1}=b(r_i-b^{l-1}S[i])+S[i+l]\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\tag{2}
$$
​		其中$l$代表的是字符串$P$的长度。$i$代表的是当前扫描到$S$字符串中的字符的下标$i\in[0,n-1]$，$n$为字符串$S$的长度。所以我们观察这个式子，我们可以发现这个式子的主要开销来源于$b^{l-1}$。计算这个式子需要$O(l-1)$的时间复杂度，但可喜的是这个值是固定的，因为带匹配字符串$P$的值是固定的，因此我们只需要预先计算好$b^{l-1}$即可。这样一来在$l$不是特别的的时候，我们通过$r_i$计算$r_{i+1}$只需要常数时间完成。注意到与预处理计算$b^{l-1}$时有一个较快的算法能在$O(\lg{l})$的时间内完成而不需要$O(l-1)$。我们可以用$Divide\space and\space conquer$分而治之的思想快速计算，为此我们注意到一个十进制数字的等于其后$k$位乘$10^j$ ($j$为该十进制数字的前$j$位)加上前$j$位，我们只需将根据$(l-1)/2$分成高位和低位分别递归计算即可。(这里不具体阐述这个算法，因为这点提速相对于整个算法的时间复杂度影响不大)。

​		如果利用式(2)，在$l$不是非常大的情况下。那么我们最终的时间复杂度为计算$d_p$的时间复杂度$\Theta(l)$，加上计算$S$中$\Theta(n-m+1)$次常数时间的迭代，以及计算$b^{l-1}$的时间$O(\lg{l})$。

​		但是需要注意到的是，我们在常数时间内计算(2)式要求我们的$d_p$不能过大($d_p$过大会导致$l$过大，**然后其值超过了计算机的字长，使得我们需要在微观层面执行多次单精度运算**)，还有就是$r_{i}$的值也不能过大。这将会导致乘法操作和减法操作的开销也是会较大的。幸运的是我们回忆起hash函数可以将较大的关键字集合映射到较小的关键字集合上，因此$Rolling\space hashing$就这么来了。

### 3 $Rolling\space Hashing$

​		我们采用除法散列法来进行散列，我们选取一个素数$q$，使得$bq$能满足一个计算机字长($\le$)(这样就可以用单精度运执行所有运算)。这样我们需要对$d_p$取模计算其$hash$值。于是我们更改算式(1)如下:
$$
\begin{equation}\qquad\qquad \qquad \qquad h(d_p)=P[m-1]+b(P[m-2]+b(P[m-3]+\dots+b(P[1]+bP[0])\dots))\space\mathrm {mod}\space q\tag{3}\qquad\qquad\qquad\qquad\
\end{equation}
$$
​		紧接着我们需要更改算式$(2)$，使得$r_i$不会过大
$$
\begin{equation}\space\space\qquad\qquad\qquad\qquad r_{i+1}=(b(r_i-f S[i])+S[i+l])\space\mathrm{mod}\space{q}\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\tag{4}
\end{equation}
$$
​		其中$f$为$b^{l-1}\space\mathrm{mod}\space q$。上述公式(4)成立运用了初等数论的知识，也就是**模$q$的等价类**，这里简单解释下，现在这里的$r_i$相对于前面公式(2)不同的地方在于，$r_i$是一个$Hash$值，假设原来代表的$b$进制数字为$r_i'$，哈希函数为$h(k)=k\space\mathrm{mod}\space q$ 。那么我们可以得到$r_i=r_i'\space\mathrm{mod}\space q$ 。于是我们就可以得到关于整数$r_i'$关于$q$的模$q$等价类(等价类的具体概念可参考离散数学)。我们记为:
$$
[r_i']_q=\{r_i'+kq:k\in\Z\}
$$
​		比如$[3]_7=\{\dots,-11,-4,3,10,17\dots\}$。等价类中数都有个共同的特点就是，他们两两之间模$q$大小是相等。我们记为$a\equiv b\pmod{q}$ ，其中$a$，$b$为两个等价类中的数。同理，我们的(4)式中的$r_i$也是$r_i'$等价类中的一员，所以我们有$r_i\equiv r_i'\pmod q$。除此之外还利用了模加法群的性质。即如果$a'\equiv a\mod{q}$，$b'\equiv b\mod{q}$。那么我们有:
$$
a'+b'\equiv(a+b)\pmod{q}
$$
​		公式(4)就是利用了这俩性质。我们不妨实验一下,假设我们当前所在的$r_i'$值为$31415$，下一个将要计算的值为$14152$，$q$为13，于是我们有:
$$
\begin{split}
14152\equiv&(31415-3*10000)*10+2\pmod {13}\\
\equiv&(7-3*3)*10+2\pmod{13}\\
\equiv&8\pmod{13}

\end{split}
$$
​		其中，第一行我们本质上直接在公式(2)上取模13，而第二行我们采用了公式$(4)$进行计算，$7$其实是$31415\space\mathrm{mod}\space{13}$的结果。也是上轮(也就是下一个计算的值为314514那一轮)我们利用$(4)$得出的值，后面的$3$也是。我们看到他们本质上是一样的。读者应该自己利用上面两个性质去证明，我这里只是简单阐述，证明过程其实非常简单。

​		有了以上的基础，我们就能开始这个算法了，我们可以知道如果$r_{i}\neq h(d_p)$，那么$r_i$所对应的子字符串必然和$P$不匹配，但如果相等，情况稍有不同，因为由于可能存在冲突，所以即使相等我们也要扫描一遍原来的字符串已确认是否是真的匹配上了。现在拿一个具体的实例来说明，假设我们的字符串P为$31415$ ，字符串$S$为$2359024141526739921$ ,$q$取$13$.具体过程像下图描绘的一样。

==实例:==

![SHashTable12](https://tva1.sinaimg.cn/large/e6c9d24egy1h0n6su7bbej20h608rt95.jpg)



​		上图中，我们首先根据$(3)$算出来$h(d_p)$，接着我们遍历字符串$S$，根据算式$(4)$计算出$r_i$，然后和$h(d_p)$进行对比，来确定是否匹配。从上图中我们可以看到，$r_6$和$r_7$都是符合要求的子字符串，但最后将只有$r_6$匹配，因为$r_{12}$对应的字符串和$P$并不相等，像这样的我们称为==伪命中点==。其实上述过程中，彩色的椭圆框住的部分像是一个滑动的窗口，这类问题都类似于滑动窗口问题($Slding\space window$)，他们的特点是窗口大小固定，然后窗口在一个范围比其大的区间上移动。我们也能看出为什么叫这种方法为滚动哈希，因为我们在窗口移动的时候，并没有重新计算交叠区域的$Hash$值，而是利用了原来的$Hash$值计算新的$Hash $值 ，这类似于滚动或滑动的过程。所以这么称呼。

==实现步骤:==

​		这个算法也叫$Rabin-Karp\space Algorithm$，它需要几步预处理，然后正式开始匹配，假设素数$q$确定，具体步骤如下:

​		(1) 首先计算字符串$P$的$hash$值，利用公式$(3)$，你需要注意的是如果字符串里面存储了字符，应该先将其转换为字符码(ASCII等)。

​		(2) 计算$b^{l-1}\space\mathrm{mod}\space q$，以及利用公式(3)计算字符串$S$前$l$个字符的$Hash$值$r_0$。

​		(3) 在字符串$S$中遍历并且利用公式(4)迭代计算$r_i,i\in[0,n-l+1]$ 。与字符串$P$的$Hash$值$h(d_p)$进行比较:

​			a. 如果相等，扫描字符串，检验是否确实为合法匹配。

​			b. 不等直接跳过。

==实现代码:==

​		简单起见我们的字符串只包括$0～9$的数字。$q$取13。

```c++
int hash(const string & s, int l)
{
    int result = s[0] - 48;
    for (size_t i = 1; i < l; i++)
    {
        result += 10 * result + (s[i] - 48);
    }
    return result % 13
}
int function(const std::string & s, const std::string p)
{
    int dp = hash(p, p.size());
    int f = std::pow(10, p.size() - 1) % 13;
    int r = hash(s, p.size() - 1);
    int result = 0;//记录匹配次数
    int i = 0;//计数
    do
    {
        if (r == dp)
        {
            size_t j = 0;
            for (; j < p.size(); i++)
            {
                if (s[i + j] != p[j])
                {
                    break;
                }
            }
            if (j== p.size())
            {
                result++;
            }
        }
        r = (10*(r - f * (s[i] - 48)) + s[i + p.size()]) % 13;
    } while (i < s.size - p.size() + 1);
    return result;
}
```

​		对代码不做阐述，具体步骤已经足够详细。

==时间复杂度:==

​		预处理时间为$\Theta(m)$。最坏时间复杂度为$\Theta((n-m+1)m)$。这是为什么呢，因为假如我们$S$中每个子字符串都匹配了，那么其时间复杂度和暴力算法其实是一样的。但我们不能以最坏情况评价这个算法。就像和散列表的时间复杂度一样，在大多数情况下，这个算法有这非常好的性能，其期望时间为$O((n-m+1)+cm)=O(n+m)$。

## 九、应用场景

​		本次讲述的为静态散列表，关键字为静态集合，还有种动态散列表，这将在高阶数据结构中讲述，其关键字集合是动态的，散列表适用于那些对查询场景较多的情况，同时这种情景并不要求数据的排列有序，因为散列表数据的排序是无序的。哈希表大量运用于文件检验以及校验用户信息，在数据库中作为索引结构大量出现，除此之外哈希表中的哈希函数在密码学中有着大量的应用。

## 十、如有错误或描述不当，请多多指教！不胜感激！

![da8152c7e7d3eac0a0dc711824fcbd22](../../Downloads/da8152c7e7d3eac0a0dc711824fcbd22.jpeg)																																																		  2022年3月24日

​                                                                                                   	                                                                     weekie_OUO